{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Autoencoder Neural Network\n","This notebook demonstrates how to create a 64x16x64 autoencoder using the provided NeuralNetwork class."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","from typing import List, Dict, Tuple, Union\n","from numpy.typing import ArrayLike\n","from nn.nn import NeuralNetwork\n","from sklearn.datasets import load_digits\n","from sklearn.model_selection import train_test_split\n","\n","# Paste the provided NeuralNetwork class here\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Define the architecture of the autoencoder\n","nn_arch = [\n","    {'input_dim': 64, 'output_dim': 16, 'activation': 'relu'},\n","    {'input_dim': 16, 'output_dim': 64, 'activation': 'sigmoid'}\n","]\n","# Initialize the NeuralNetwork with the architecture\n","autoencoder = NeuralNetwork(nn_arch=nn_arch, lr=0.01, seed=42, batch_size=64, epochs=100, loss_function='mse')"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare the Dataset\n","Here, you would typically load and preprocess your dataset. For demonstration, we will create a synthetic dataset."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["64\n"]}],"source":["# Create a synthetic dataset\n","dataset = load_digits()\n","print(len(dataset[\"data\"][0]))\n","X_train, X_test, y_train, y_test = train_test_split(dataset[\"data\"], dataset[\"target\"], test_size = 0.2)"]},{"cell_type":"markdown","metadata":{},"source":["## Train the Autoencoder\n","Now, we train the autoencoder using the synthetic dataset."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X.T.SHAPE: (64, 1437)\n","idx:  0\n","idx:  1\n","X.T.SHAPE: (64, 360)\n","idx:  0\n","idx:  1\n","y.shape: (1437,)\n","y.shape: (360,)\n","y.shape: (1437,)\n"]},{"ename":"IndexError","evalue":"list index out of range","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32m/Users/siddharthmahesh/Documents/BMI_203/final-nn/autoencoder_neural_network.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/siddharthmahesh/Documents/BMI_203/final-nn/autoencoder_neural_network.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Training the autoencoder\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/siddharthmahesh/Documents/BMI_203/final-nn/autoencoder_neural_network.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_loss, val_loss \u001b[39m=\u001b[39m autoencoder\u001b[39m.\u001b[39;49mfit(X_train, y_train, X_test, y_test)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/siddharthmahesh/Documents/BMI_203/final-nn/autoencoder_neural_network.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Optionally, plot the training and validation loss over epochs\u001b[39;00m\n","File \u001b[0;32m~/Documents/BMI_203/final-nn/nn/nn.py:282\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    279\u001b[0m per_epoch_loss_val\u001b[39m.\u001b[39mappend(loss_val)\n\u001b[1;32m    281\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m grads \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackprop(y_train, y_hat_train, cache_train)\n\u001b[1;32m    283\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_params(grads)\n\u001b[1;32m    285\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss_train\u001b[39m}\u001b[39;00m\u001b[39m, Val Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss_val\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/Documents/BMI_203/final-nn/nn/nn.py:215\u001b[0m, in \u001b[0;36mNeuralNetwork.backprop\u001b[0;34m(self, y, y_hat, cache)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mfor\u001b[39;00m layer_idx \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39march))):\n\u001b[1;32m    211\u001b[0m     layer_idx \u001b[39m=\u001b[39m layer_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    212\u001b[0m     dA_prev, dW_curr, db_curr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_single_backprop(\n\u001b[1;32m    213\u001b[0m         W_curr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param_dict[\u001b[39m'\u001b[39m\u001b[39mW\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(layer_idx)],  \u001b[39m# Extract current layer weights and biases\u001b[39;00m\n\u001b[1;32m    214\u001b[0m         b_curr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param_dict[\u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(layer_idx)],\n\u001b[0;32m--> 215\u001b[0m         Z_curr\u001b[39m=\u001b[39mcache[\u001b[39m'\u001b[39;49m\u001b[39mZ\u001b[39;49m\u001b[39m'\u001b[39;49m][layer_idx],\n\u001b[1;32m    216\u001b[0m         A_prev\u001b[39m=\u001b[39mcache[\u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m][layer_idx],\n\u001b[1;32m    217\u001b[0m         dA_curr\u001b[39m=\u001b[39mdA_curr,\n\u001b[1;32m    218\u001b[0m         activation_curr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39march[layer_idx][\u001b[39m'\u001b[39m\u001b[39mactivation\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    219\u001b[0m     )\n\u001b[1;32m    221\u001b[0m     grad_dict[\u001b[39m'\u001b[39m\u001b[39mdW\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(layer_idx)] \u001b[39m=\u001b[39m dW_curr\n\u001b[1;32m    222\u001b[0m     grad_dict[\u001b[39m'\u001b[39m\u001b[39mdb\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(layer_idx)] \u001b[39m=\u001b[39m db_curr\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}],"source":["# Training the autoencoder\n","train_loss, val_loss = autoencoder.fit(X_train, y_train, X_test, y_test)\n","\n","# Optionally, plot the training and validation loss over epochs"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":4}
